{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LeibGit/-DI_Bootcamp/blob/main/Copy_of_Exercises_XP_student.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2d73857",
      "metadata": {
        "id": "d2d73857"
      },
      "source": [
        "# Exercises XP: Guided Student Notebook\n",
        "\n",
        "This guided notebook follows the **content on the platform**. Some cells are **prefilled** and must be **executed only**. Cells marked **To-Do** require your action. When a written answer is required, the **To-Do** appears inside a markdown cell. When code is required, the **To-Do** appears inside a code cell as comments.\n",
        "\n",
        "**Learning points** appear when a concept is important for intuition or to help you learn new tricks/concepts related to other AI topics.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c5a14c7",
      "metadata": {
        "id": "3c5a14c7"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "**What you will learn**  \n",
        "- The fundamentals of deep learning and neural networks.\n",
        "- How to build and train simple neural networks using TensorFlow or Keras.\n",
        "- The concepts of forward and backward propagation.\n",
        "- How to visualize and interpret model predictions.\n",
        "\n",
        "**What you will create**  \n",
        "- A simple perceptron-based decision system.\n",
        "- A neural network for classifying handwritten digits from the MNIST dataset.\n",
        "- A forward propagation calculation for predicting house prices.\n",
        "- A Python implementation of forward and backward propagation.\n",
        "- Visualizations of predictions made by a neural network on the MNIST dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81d5c20b",
      "metadata": {
        "id": "81d5c20b"
      },
      "source": [
        "## ğŸŒŸ Exercise 1: Small Quiz\n",
        "\n",
        "**As stated in the instructions**  \n",
        "- What is the key difference between traditional machine learning and deep learning?  \n",
        "- How do artificial neural networks mimic the human brain?  \n",
        "- Why does deep learning perform better on large datasets compared to traditional machine learning?  \n",
        "- What are some challenges of deep learning, and how can they be addressed?  \n",
        "- What is feature engineering, and why is it not needed in deep learning?  \n",
        "- What role do hidden layers play in a deep learning model?  \n",
        "- In an artificial neural network, what is the function of an activation function?\n",
        "\n",
        "**Guidance**  \n",
        "Answer in full sentences. Use concise definitions first, then add one concrete example for any two questions to demonstrate understanding.\n",
        "\n",
        "**To-Do:** Write your answers below.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. machine learning can not handle audio, images, and more intensive problems where we may not know all features needed to get a desired output.\n",
        "2. they have nuerons that fire and communi.cate in a similiar way as the brain does to make predictions and assumptions.\n",
        "3. Deep learning is able to make much better predictions given more data becuase it tweaks the weights.\n",
        "4. Core challenges are figuring out epochs\n",
        "5. Feature engineering is not needed in deep learning because the model creates the features itself by adjusting weights and deciding what is important to get a desired output.\n",
        "6. hidden layers are good for making connections for complex pieces of data.\n",
        "7. Relu"
      ],
      "metadata": {
        "id": "X5UbLQKbNzlc"
      },
      "id": "X5UbLQKbNzlc"
    },
    {
      "cell_type": "markdown",
      "id": "ab821822",
      "metadata": {
        "id": "ab821822"
      },
      "source": [
        "## ğŸŒŸ Exercise 2: Building a Simple Perceptron Decision System\n",
        "\n",
        "**As stated in the instructions**  \n",
        "You will create a perceptron-based decision system to decide whether you should go outside based on two inputs:\n",
        "- Temperature in Â°F\n",
        "- Rainy where Yes is 1 and No is 0\n",
        "\n",
        "Given parameters: Temperature weight 0.6, Rain weight 0.4, Bias 2. Compute the weighted sum\n",
        "\\[ \\text{Weighted Sum} = 0.6\\cdot \\text{Temperature} + 0.4\\cdot \\text{Rain} + \\text{Bias} \\]\n",
        "Apply a step activation:\n",
        "- If Weighted Sum > 20, output 1\n",
        "- If Weighted Sum \\le 20, output 0\n",
        "\n",
        "Evaluate:  \n",
        "Case 1: Temperature 70Â°F, Rain 0  \n",
        "Case 2: Temperature 50Â°F, Rain 1\n",
        "\n",
        "**Guidance**  \n",
        "Implement the weighted sum function yourself. Use a reusable step activation. Then compute the two cases and print the decision for each.\n",
        "\n",
        "**Learning point**  \n",
        "A perceptron implements a linear decision boundary. The bias shifts the threshold. Use step activation for simple binary rules. Prefer differentiable activations when training with gradient descent.\n",
        "\n",
        "![image.png](https://github.com/user-attachments/assets/3c4f3261-164d-4039-8e1c-aff2db7162dd)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "4e813f23",
      "metadata": {
        "id": "4e813f23"
      },
      "outputs": [],
      "source": [
        "# PREFILLED: just execute\n",
        "from typing import Tuple\n",
        "\n",
        "W_TEMP = 0.6\n",
        "W_RAIN = 0.4\n",
        "BIAS   = 2.0\n",
        "\n",
        "def step_activation(s: float, threshold: float = 20.0) -> int:\n",
        "    \"\"\"Return 1 if s > threshold else 0.\"\"\"\n",
        "    return 1 if s > threshold else 0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "1043126a",
      "metadata": {
        "id": "1043126a",
        "outputId": "6d5e450d-6c30-4433-c323-9bc5708a108b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'case1': {'sum': 44.0, 'decision': 1}, 'case2': {'sum': 32.4, 'decision': 1}}\n"
          ]
        }
      ],
      "source": [
        "# To-Do: implement weighted_sum then use it to compute the two cases\n",
        "def weighted_sum(temperature_f: float, rain01: int) -> float:\n",
        "    \"\"\"Compute s = 0.6*Temperature + 0.4*Rain + Bias.\"\"\"\n",
        "    ws = W_TEMP * temperature_f + W_RAIN * rain01 + BIAS\n",
        "    return ws\n",
        "\n",
        "# To-Do: compute decisions for the two cases and print them\n",
        "s1 = weighted_sum(70, 0)  # Temperature=70, Rain=0\n",
        "s2 = weighted_sum(50, 1)  # Temperature=50, Rain=1\n",
        "\n",
        "y1 = step_activation(s1)\n",
        "y2 = step_activation(s2)\n",
        "\n",
        "print({\"case1\": {\"sum\": s1, \"decision\": y1}, \"case2\": {\"sum\": s2, \"decision\": y2}})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cfd74d8d",
      "metadata": {
        "id": "cfd74d8d"
      },
      "source": [
        "**To-Do:** Interpret your results. Did the perceptron suggest going outside in both cases? Why or why not? Explain using the threshold rule and the effect of bias.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It did suggest to go outside in both cases. becuase the model is only supposed to suggest staying inside if the sum was less than 20 which was not the case."
      ],
      "metadata": {
        "id": "extbfloPjMWf"
      },
      "id": "extbfloPjMWf"
    },
    {
      "cell_type": "markdown",
      "id": "763ce105",
      "metadata": {
        "id": "763ce105"
      },
      "source": [
        "## ğŸŒŸ Exercise 3: Building a Simple Neural Network with TensorFlow or Keras\n",
        "\n",
        "**As stated in the instructions**  \n",
        "Build a classifier for MNIST with:\n",
        "- One input layer\n",
        "- One hidden layer with 128 neurons and ReLU activation\n",
        "- One output layer with 10 neurons and softmax activation\n",
        "\n",
        "Steps:\n",
        "1. Import TensorFlow and needed modules from `tensorflow.keras`\n",
        "2. Load the MNIST dataset\n",
        "3. Normalize data by dividing by 255.0\n",
        "4. One-hot encode the labels\n",
        "5. Build a sequential model with Flatten, Dense(128, ReLU), Dense(10, softmax)\n",
        "6. Compile with optimizer `adam`, loss `categorical_crossentropy`, metrics `accuracy`\n",
        "7. Train and evaluate on the test set\n",
        "\n",
        "**Guidance**  \n",
        "You will execute prefilled boilerplate for imports and dataset loading. You will complete the one-hot encoding and the model specification, compile, train, and evaluate.\n",
        "\n",
        "**Learning point**  \n",
        "Normalization stabilizes optimization. One-hot encoding matches softmax outputs to target vectors. ReLU accelerates training by avoiding saturation, while softmax maps logits to a probability simplex.\n",
        "\n",
        "![image.png](https://github.com/user-attachments/assets/61737b80-69cb-41a1-aea5-76092cd2a373)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "45ea6cf7",
      "metadata": {
        "id": "45ea6cf7",
        "outputId": "7b45021b-9a65-44e3-94fc-86892b37c251",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow: 2.19.0\n",
            "Train shape: (60000, 28, 28) Test shape: (10000, 28, 28)\n"
          ]
        }
      ],
      "source": [
        "# PREFILLED: just execute\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "print(\"TensorFlow:\", tf.__version__)\n",
        "\n",
        "# Load data\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Normalize to [0,1]\n",
        "x_train = x_train.astype(\"float32\") / 255.0\n",
        "x_test  = x_test.astype(\"float32\") / 255.0\n",
        "\n",
        "# Flatten will be added in the model; keep images as 28x28 for now\n",
        "print(\"Train shape:\", x_train.shape, \"Test shape:\", x_test.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "075081d1",
      "metadata": {
        "id": "075081d1",
        "outputId": "63bbd562-fff5-42bc-e878-7f6870781389",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Labels one-hot: (60000, 10) (10000, 10)\n"
          ]
        }
      ],
      "source": [
        "# To-Do: one-hot encode labels y_train and y_test using to_categorical\n",
        "# Hints: use to_categorical\n",
        "y_train_oh = to_categorical(y_train)\n",
        "y_test_oh  = to_categorical(y_test)\n",
        "\n",
        "print(\"Labels one-hot:\", y_train_oh.shape, y_test_oh.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "0ada4e03",
      "metadata": {
        "id": "0ada4e03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "outputId": "ea937119-18a4-4b52-f762-1d7fe5cfb0eb"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
              "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
              "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
              "â”‚ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)             â”‚ ?                      â”‚   \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ ?                      â”‚   \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ ?                      â”‚   \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
              "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
              "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
              "â”‚ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             â”‚ ?                      â”‚   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ ?                      â”‚   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ ?                      â”‚   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# To-Do:  Build a sequential model with Flatten, Dense(128, ReLU), Dense(10, softmax)\n",
        "model = models.Sequential([\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dense(10, activation='softmax'),\n",
        "])\n",
        "\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "9da50234",
      "metadata": {
        "id": "9da50234"
      },
      "outputs": [],
      "source": [
        "# To-Do: compile the model with adam, categorical_crossentropy, and accuracy\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "7a07bd5e",
      "metadata": {
        "id": "7a07bd5e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b83c97ba-428b-4ec9-f5e4-3bf2d87cb96c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.8748 - loss: 0.4404 - val_accuracy: 0.9682 - val_loss: 0.1184\n",
            "Epoch 2/5\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9623 - loss: 0.1287 - val_accuracy: 0.9738 - val_loss: 0.0924\n",
            "Epoch 3/5\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.9753 - loss: 0.0833 - val_accuracy: 0.9748 - val_loss: 0.0882\n",
            "Epoch 4/5\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9811 - loss: 0.0606 - val_accuracy: 0.9777 - val_loss: 0.0793\n",
            "Epoch 5/5\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9874 - loss: 0.0443 - val_accuracy: 0.9767 - val_loss: 0.0826\n",
            "\u001b[1m313/313\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9689 - loss: 0.0946\n",
            "{'test_loss': 0.08099876344203949, 'test_acc': 0.9743000268936157}\n"
          ]
        }
      ],
      "source": [
        "# To-Do: train then evaluate\n",
        "history = model.fit(\n",
        "    x_train,\n",
        "    y_train_oh,\n",
        "    epochs=5,\n",
        "    batch_size=32,\n",
        "    validation_split=0.1,\n",
        "    verbose=1\n",
        ")\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test_oh, verbose=1)\n",
        "print({\"test_loss\": float(test_loss), \"test_acc\": float(test_acc)})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b07b6eb",
      "metadata": {
        "id": "1b07b6eb"
      },
      "source": [
        "## ğŸŒŸ Exercise 4: Forward Propagation Calculation\n",
        "\n",
        "**As stated in the instructions**  \n",
        "Predict house prices given:\n",
        "- Inputs: xâ‚ = 2000 square feet, xâ‚‚ = 3 bedrooms\n",
        "- Weights: wâ‚ = 0.5, wâ‚‚ = 0.7\n",
        "- Bias: b = 50,000\n",
        "- Activation: ReLU\n",
        "\n",
        "Compute z before activation, apply ReLU, and interpret the prediction.\n",
        "\n",
        "**Guidance**  \n",
        "Write scalar computations. Then reason about the role of the bias and ReLU on the output scale.\n",
        "\n",
        "**Learning point**  \n",
        "ReLU passes positive linear combinations unchanged and zeros out negatives. Bias sets the baseline prediction independent of inputs.\n",
        "![](./figs/1.gif)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "0d7a303c",
      "metadata": {
        "id": "0d7a303c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98abca6b-af26-495f-b3c7-08990bd03236"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'z': 51002.1, 'prediction': np.float64(51002.1)}\n"
          ]
        }
      ],
      "source": [
        "# To-Do: compute forward propagation for the given numbers\n",
        "x1, x2 = 2000.0, 3.0\n",
        "w1, w2 = 0.5, 0.7\n",
        "b = 50000.0\n",
        "\n",
        "# To-Do: compute z = x1*w1 + x2*w2 + b\n",
        "z = x1*w1 + x2*w2 + b\n",
        "\n",
        "# To-Do: apply ReLU\n",
        "y = np.maximum(0, z)\n",
        "\n",
        "print({\"z\": z, \"prediction\": y})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a37d5d3b",
      "metadata": {
        "id": "a37d5d3b"
      },
      "source": [
        "**To-Do:** Interpret the result as a house price. Explain the influence of input scaling and bias in full sentences.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ReLu allows it to pass through since it is not zero."
      ],
      "metadata": {
        "id": "oktRHFqjtmyq"
      },
      "id": "oktRHFqjtmyq"
    },
    {
      "cell_type": "markdown",
      "id": "64b99260",
      "metadata": {
        "id": "64b99260"
      },
      "source": [
        "## ğŸŒŸ Exercise 5 (optional): Implementing Forward and Backward Propagation in Python\n",
        "\n",
        "**As stated in the enoncÃ©**  \n",
        "You will code a simple neural network that performs forward propagation and backpropagation for a regression problem predicting exam scores from study hours.\n",
        "\n",
        "**Guidance**  \n",
        "Run the prefilled code. Then modify the learning rate or initial weights and observe changes in the loss and updates.\n",
        "Provide a short written explanation.\n",
        "\n",
        "**Learning point**  \n",
        "Gradient descent updates parameters in the direction of steepest loss reduction.\n",
        "The learning rate controls step size. Too small slows learning. Too large risks divergence.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "c4695c04",
      "metadata": {
        "id": "c4695c04",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5abe45c4-d55b-4e66-8eb1-f3ba4d6ca3f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Prediction: 74.4\n",
            "Loss: 56.17999999999994\n",
            "Updated Weights: [0.524 9.28 ]\n",
            "Updated Bias: 10.106\n"
          ]
        }
      ],
      "source": [
        "# PREFILLED: just execute\n",
        "import numpy as np\n",
        "\n",
        "# Initialize input data (features)\n",
        "x = np.array([4, 80])  # 4 hours studied, previous test score: 80\n",
        "\n",
        "# Initialize weights and bias\n",
        "w = np.array([0.1, 0.8])  # Initial weights\n",
        "b = 10  # Initial bias\n",
        "\n",
        "# Forward Propagation\n",
        "def forward_propagation(x, w, b):\n",
        "    z = np.dot(x, w) + b  # Weighted sum\n",
        "    return z  # Linear activation (No ReLU here, it's a regression task)\n",
        "\n",
        "# Compute prediction\n",
        "y_pred = forward_propagation(x, w, b)\n",
        "y_true = 85  # Actual exam score\n",
        "\n",
        "# Compute Loss (Mean Squared Error)\n",
        "loss = 0.5 * (y_true - y_pred) ** 2\n",
        "\n",
        "# Compute Gradients\n",
        "grad_w = -(y_true - y_pred) * x  # Partial derivatives with respect to weights\n",
        "grad_b = -(y_true - y_pred)  # Partial derivative with respect to bias\n",
        "\n",
        "# Update Weights and Bias\n",
        "learning_rate = 0.01\n",
        "w_new = w - learning_rate * grad_w\n",
        "b_new = b - learning_rate * grad_b\n",
        "\n",
        "# Print Results\n",
        "print(\"Initial Prediction:\", y_pred)\n",
        "print(\"Loss:\", loss)\n",
        "print(\"Updated Weights:\", w_new)\n",
        "print(\"Updated Bias:\", b_new)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d688ea3d",
      "metadata": {
        "id": "d688ea3d"
      },
      "source": [
        "**To-Do:** Explain in full sentences why updating weights using gradient descent reduces the error. Then modify the learning rate or initial weights and describe how this affects learning.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "adjusting the wieghts changes the importance of hours study in relation to the score."
      ],
      "metadata": {
        "id": "AC7a6-Aqu9mX"
      },
      "id": "AC7a6-Aqu9mX"
    },
    {
      "cell_type": "markdown",
      "id": "40b040e0",
      "metadata": {
        "id": "40b040e0"
      },
      "source": [
        "## ğŸŒŸ Exercise 6 (optional): Visualizing Predictions on MNIST\n",
        "\n",
        "**As stated in the enoncÃ©**  \n",
        "Train a simple model on MNIST, then visualize predictions for some test images with predicted labels.\n",
        "\n",
        "**Guidance**  \n",
        "Reuse the model from Exercise 3 if still in memory. Otherwise re-run Exercise 3. Plot a grid of test images with predicted labels and optionally true labels.\n",
        "\n",
        "**Learning point**  \n",
        "Softmax outputs are probabilities over classes. Inspect the confidence values. Visual errors often indicate confusions between similar digits.\n",
        "\n",
        "![image.png](https://github.com/user-attachments/assets/e00d3913-55d4-4b62-8f70-1216bcc2e4e4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "3da295b5",
      "metadata": {
        "id": "3da295b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "outputId": "b0d8f007-10b4-4229-fc32-1378d5897666"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Unrecognized data type: x=7955 (of type <class 'int'>)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1309900122.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# To-Do: get predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0my_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/trainers/data_adapters/__init__.py\u001b[0m in \u001b[0;36mget_data_adapter\u001b[0;34m(x, y, sample_weight, batch_size, steps_per_epoch, shuffle, class_weight)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;31m# )\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Unrecognized data type: x={x} (of type {type(x)})\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Unrecognized data type: x=7955 (of type <class 'int'>)"
          ]
        }
      ],
      "source": [
        "# To-Do: visualize a few predictions with matplotlib\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Ensure we have the model and x_test, y_test from Exercise 3\n",
        "# To-Do: select N samples from x_test by using np.random.choice\n",
        "N_SAMPLES = 12 # Define the number of samples to visualize\n",
        "idx = np.random.choice(len(x_test), N_SAMPLES, replace=False) # Get N_SAMPLES random indices\n",
        "x_vis = x_test[idx] # Get the actual image data for these indices\n",
        "y_true = y_test[idx] # Get the true labels for these indices\n",
        "\n",
        "# To-Do: get predictions\n",
        "y_prob = model.predict(x_vis) # Make predictions on the selected image data\n",
        "y_pred = np.argmax(y_prob, axis=1) # Get the predicted class for each image\n",
        "\n",
        "#plotting\n",
        "cols = 6\n",
        "rows = int(np.ceil(N_SAMPLES/cols)) # Use N_SAMPLES here\n",
        "plt.figure(figsize=(12, 2*rows))\n",
        "for i in range(N_SAMPLES): # Iterate through the selected samples\n",
        "    plt.subplot(rows, cols, i + 1)\n",
        "    plt.imshow(x_vis[i], cmap=\"gray\") # Plot the i-th selected image\n",
        "    plt.title(f\"pred={y_pred[i]} true={y_true[i]}\" )\n",
        "    plt.axis(\"off\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "mlflow",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}