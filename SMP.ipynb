{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "V5E1",
      "authorship_tag": "ABX9TyO98+IXNsqSF7uLm+SJOWzp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LeibGit/-DI_Bootcamp/blob/main/SMP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2_SaqR_pDetO"
      },
      "outputs": [],
      "source": [
        "# Extract zip file with stock data\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "zip_file_path = \"archive.zip\"\n",
        "extracted_dir = \"stock_data\"\n",
        "\n",
        "if not os.path.exists(extracted_dir):\n",
        "  os.makedirs(extracted_dir)\n",
        "\n",
        "try:\n",
        "  with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extracted_dir)\n",
        "  print(f\"Files extracted in: {extracted_dir}\")\n",
        "except zipfile.BadZipFile as e:\n",
        "  print(f\"An error occured with the zip file: {e}\")\n",
        "except FileNotFoundError as e:\n",
        "  print(f\"File not found: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# make dowloads\n",
        "!pip install gensim\n",
        "!pip install spacy\n",
        "!pip install numpy\n",
        "!pip install tensorflow"
      ],
      "metadata": {
        "id": "QN3NNfWwIcqx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# importing modules\n",
        "import spacy\n",
        "import gensim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('fivethirtyeight')\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, Dropout, GRU\n",
        "from keras.optimizers import SGD\n",
        "import math\n",
        "from sklearn.metrics import mean_squared_error"
      ],
      "metadata": {
        "id": "d8gPkwWDIADJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# grab sample columns that csv contains for necessary metadata creation in master dataframe\n",
        "df = pd.read_csv(\"stock_data/stocks/AAPL.csv\")\n",
        "df.columns"
      ],
      "metadata": {
        "id": "jrpnjo-kL4qW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -- EXPECT A 2-Minute COMPUTE TIME --\n",
        "\n",
        "# Loop through all assets and convert into one main dataframe\n",
        "from pathlib import Path\n",
        "import glob\n",
        "\n",
        "# list of all dataframes\n",
        "all_dfs = []\n",
        "\n",
        "# main paths for both stocks and etfs\n",
        "main_path_stocks = \"stock_data/stocks\"\n",
        "main_path_etfs = \"stock_data/etfs\"\n",
        "\n",
        "# finding all files that end in csv\n",
        "etf_csv_files = glob.glob(os.path.join(main_path_etfs, \"*.csv\"))\n",
        "stocks_csv_files = glob.glob(os.path.join(main_path_stocks, \"*.csv\"))\n",
        "\n",
        "# loop for etfs\n",
        "for filename in etf_csv_files:\n",
        "  try:\n",
        "    df = pd.read_csv(filename)\n",
        "    df[\"ticker\"] = Path(filename).stem #grabs ticker symbol by removing .csv from filename\n",
        "    df[\"asset_type\"] = \"etf\"\n",
        "    all_dfs.append(df)\n",
        "  except Exception as e:\n",
        "    continue\n",
        "\n",
        "#loop for stocks\n",
        "for filename in stocks_csv_files:\n",
        "  try:\n",
        "    df = pd.read_csv(filename)\n",
        "    df[\"ticker\"] = Path(filename).stem #grabs ticker symbol by removing .csv from filename\n",
        "    df[\"asset_type\"] = \"stock\"\n",
        "    all_dfs.append(df)\n",
        "  except Exception as e:\n",
        "    continue"
      ],
      "metadata": {
        "id": "kj44DagPJY8l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# combine into one dataframe\n",
        "combined_df = pd.concat(all_dfs, ignore_index=True)\n",
        "print(\"Successfully combined all files into one DataFrame.\")\n",
        "# analyze first 5 rows to confirm\n",
        "combined_df.head()"
      ],
      "metadata": {
        "id": "xGkrbPLFRdIr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#aanalyze columns to confirm\n",
        "combined_df.columns"
      ],
      "metadata": {
        "id": "e-D1aTZkQyat"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# add filler column for next days predicted close price\n",
        "combined_df[\"Predicted_close\"] = np.nan\n",
        "combined_df.columns"
      ],
      "metadata": {
        "id": "PcIbNBP5TukV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get range of dates to decide data split\n",
        "print(np.max(combined_df[\"Date\"]))\n",
        "print(np.min(combined_df[\"Date\"]))"
      ],
      "metadata": {
        "id": "jM3sGedwxpUI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure Date is in datetime format\n",
        "combined_df[\"Date\"] = pd.to_datetime(combined_df[\"Date\"])\n",
        "\n",
        "# Create a target column (Next day's close price)\n",
        "combined_df[\"Predicted_close\"] = combined_df[\"Close\"].shift(-1)\n",
        "\n",
        "# Drop the last row(s) with NaN target\n",
        "combined_df = combined_df.dropna(subset=[\"Predicted_close\"])\n",
        "\n",
        "# Split by date\n",
        "split_date = pd.Timestamp('2019-04-02')\n",
        "\n",
        "train_df = combined_df.loc[combined_df['Date'] <= split_date]\n",
        "test_df  = combined_df.loc[combined_df['Date'] > split_date]\n",
        "\n",
        "# Select features and target\n",
        "features = [\"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]\n",
        "\n",
        "X_train = train_df[features]\n",
        "y_train = train_df[\"Predicted_close\"]\n",
        "\n",
        "X_test  = test_df[features]\n",
        "y_test  = test_df[\"Predicted_close\"]\n"
      ],
      "metadata": {
        "id": "d3OybCMuSycY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# scale features between 0 and 1\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))"
      ],
      "metadata": {
        "id": "r18rWnMa2yH6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize the STM model with Sequential\n",
        "regressor = Sequential()\n",
        "\n",
        "# First LSTM layer\n",
        "regressor.add(\n",
        "    LSTM(\n",
        "        units=50, # neurons => memory cells\n",
        "        return_sequences=True, # return sequence for future layers\n",
        "        input_shape=(X_train.shape[1], 1) #\n",
        "      )\n",
        "    )\n",
        "regressor.add(Dropout(0.2))\n",
        "\n",
        "# Second LSTM layer\n",
        "regressor.add(\n",
        "    LSTM(\n",
        "        units=50,\n",
        "        return_sequences=True\n",
        "        )\n",
        "    )\n",
        "regressor.add(Dropout(0.2))\n",
        "\n",
        "# Third LSTM layer\n",
        "regressor.add(\n",
        "    LSTM(\n",
        "        units=50,\n",
        "        return_sequences=True\n",
        "    )\n",
        ")\n",
        "regressor.add(Dropout(0.2))\n",
        "\n",
        "# Fourth LSTM layer\n",
        "regressor.add(LSTM(units=50))\n",
        "regressor.add(Dropout(0.2))\n",
        "\n",
        "# Output layer\n",
        "regressor.add(Dense(units=1))\n",
        "\n",
        "# Compiling the model\n",
        "regressor.compile(optimizer=\"rmsprop\", loss=\"mean_squared_error\")\n",
        "\n",
        "# Training the model\n",
        "regressor.fit(X_train, y_train, epochs=50, batch_size=32)"
      ],
      "metadata": {
        "id": "iSFqFOkez16e"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}