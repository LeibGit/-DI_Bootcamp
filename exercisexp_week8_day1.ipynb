{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOGW1X8Zq6fT9vKqZAZzQ0d",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LeibGit/-DI_Bootcamp/blob/main/exercisexp_week8_day1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Exercise 1***"
      ],
      "metadata": {
        "id": "LYDdrVrFOkTW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# add all imports\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import pos_tag\n",
        "import spacy\n",
        "\n",
        "nltk.download(\"averaged_perceptron_tagger_eng\")\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"punkt_tab\")"
      ],
      "metadata": {
        "id": "6iVvjpc53Dva"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2BoC2hi7249M"
      },
      "outputs": [],
      "source": [
        "# intiialize sample data\n",
        "data = {\n",
        "    'Review': [\n",
        "        'At McDonald\\'s the food was ok and the service was bad.',\n",
        "        'I would not recommend this Japanese restaurant to anyone.',\n",
        "        'I loved this restaurant when I traveled to Thailand last summer.',\n",
        "        'The menu of Loving has a wide variety of options.',\n",
        "        'The staff was friendly and helpful at Google\\'s employees restaurant.',\n",
        "        'The ambiance at Bella Italia is amazing, and the pasta dishes are delicious.',\n",
        "        'I had a terrible experience at Pizza Hut. The pizza was burnt, and the service was slow.',\n",
        "        'The sushi at Sushi Express is always fresh and flavorful.',\n",
        "        'The steakhouse on Main Street has a cozy atmosphere and excellent steaks.',\n",
        "        'The dessert selection at Sweet Treats is to die for!'\n",
        "    ]\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NlpPipeline:\n",
        "\n",
        "  def __init__(self,text: str):\n",
        "    self.text = text\n",
        "    self.nlp = spacy.load(\"en_core_web_sm\")\n",
        "    self.lemmatized_text = None\n",
        "\n",
        "  # preprocess text function\n",
        "  def preprocess_text(self):\n",
        "\n",
        "    # tokenize the lowercase, anti punctuation converted text\n",
        "    tokenize = word_tokenize(\n",
        "        self.text.lower().replace(\"?\", \"\").replace(\".\", \"\").replace(\"!\", \"\")\n",
        "    )\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    filtered_words = [word for word in tokenize if word not in stop_words]\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "    lemmatized_text = nlp(\" \".join(filtered_words)) # Join the list back into a string\n",
        "    print(type(lemmatized_text))\n",
        "    self.lemmatized_text = lemmatized_text\n",
        "    return lemmatized_text\n",
        "\n",
        "  def show_preprocessed_res(self):\n",
        "    # print('---------------', type(self.text), self.text)\n",
        "    result = self.preprocess_text()\n",
        "    return result\n",
        "\n",
        "  def create_new_data(self):\n",
        "    # create a new dataset with clean data\n",
        "    cleaned_data = {}\n",
        "    cleaned_data[\"Review\"] = self.preprocess_text\n",
        "    print(cleaned_data)\n",
        "\n",
        "  # natural entity recognition\n",
        "  def perform_ner(self):\n",
        "\n",
        "    doc = self.nlp(self.lemmatized_text)\n",
        "\n",
        "    for ent in doc.ents:\n",
        "      print(f\"Text: {ent.text} | Label: {ent.label_}\")\n",
        "\n",
        "  def perform_pos_tagging(self):\n",
        "    tagged =[token.text for token in self.lemmatized_text]\n",
        "    return tagged"
      ],
      "metadata": {
        "id": "QqIzaCIKDeP1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for sentence in data[\"Review\"]:\n",
        "  new_instance = NlpPipeline(sentence)\n",
        "  new_instance.preprocess_text()\n",
        "  # The following calls might also need adjustment based on their definitions\n",
        "  # and what they are supposed to do with the instance's state.\n",
        "  # For now, let's just fix the immediate error.\n",
        "  print(new_instance.show_preprocessed_res())\n",
        "  print(new_instance.create_new_data())\n",
        "  print(new_instance.perform_ner())\n",
        "  print(new_instance.perform_pos_tagging())"
      ],
      "metadata": {
        "id": "3urm2Jm-FtWT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Exercise 2***"
      ],
      "metadata": {
        "id": "ClvhU-k1Oed1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "id": "lDU0IvhYQ6of"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"punkt_tab\")"
      ],
      "metadata": {
        "id": "decW9ovjQLqC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_sentences = [new_instance.preprocess_text()]\n",
        "\n",
        "model = Word2Vec(sentences=preprocessed_sentences, vector_size=100, window=5, min_count=1, workers=4, sg=1)\n",
        "print(model)\n",
        "print(model.wv.vector_size)\n",
        "# indicates an 100 dimensional vector"
      ],
      "metadata": {
        "id": "TcbnDjwSRLAX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "import numpy as np\n",
        "\n",
        "def plot_word_embeddings(model_wv):\n",
        "  # Get all words from the vocabulary and their corresponding vectors\n",
        "  words = list(model_wv.key_to_index.keys())\n",
        "  vectors = model_wv[words]\n",
        "\n",
        "  # Reduce dimensions to 2 using PCA for visualization\n",
        "  pca = PCA(n_components=2)\n",
        "  result = pca.fit_transform(vectors)\n",
        "\n",
        "  # Plot the reduced vectors\n",
        "  plt.figure(figsize=(12, 10))\n",
        "  plt.scatter(result[:, 0], result[:, 1])\n",
        "\n",
        "  # Annotate each point with its word\n",
        "  for i, word in enumerate(words):\n",
        "    plt.annotate(word, xy=(result[i, 0], result[i, 1]), xytext=(5, 2),\n",
        "                 textcoords='offset points', ha='right', va='bottom')\n",
        "\n",
        "  plt.title(\"Word Embeddings (2D PCA)\")\n",
        "  plt.xlabel(\"PCA Component 1\")\n",
        "  plt.ylabel(\"PCA Component 2\")\n",
        "  plt.grid(True)\n",
        "  plt.show()\n",
        "\n",
        "# Call the function with the word vectors (model.wv) from the trained model\n",
        "# Assuming 'model' is the Word2Vec model trained in the previous cell\n",
        "plot_word_embeddings(model.wv)"
      ],
      "metadata": {
        "id": "Ayd4V3zEYRTW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}