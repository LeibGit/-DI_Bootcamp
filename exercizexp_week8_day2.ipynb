{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyM/qoLZbNsezz3JVSJ6xQ3h",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LeibGit/-DI_Bootcamp/blob/main/exercizexp_week8_day2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dI_bxBiymFe8"
      },
      "outputs": [],
      "source": [
        "!pip install faiss-cpu\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import faiss\n",
        "import uuid\n",
        "import json\n",
        "from chromadb.config import Settings\n",
        "from sentence_transformers import SentenceTransformer, InputExample\n",
        "import chromadb\n",
        "from chromadb.config import Settings\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pdf = pd.read_csv(\"labelled_newscatcher_dataset.csv\", sep=';')\n",
        "pdf.head()"
      ],
      "metadata": {
        "id": "j-_KN1z7oVYb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pdf.index = range(1, len(pdf) +1)\n",
        "pdf.index.name = \"id\"\n",
        "pdf.head()"
      ],
      "metadata": {
        "id": "iuAul__kpeH3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(pdf)"
      ],
      "metadata": {
        "id": "aXyoApXYuj44"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sub_ds = pdf[:1_000]\n",
        "display(sub_ds)"
      ],
      "metadata": {
        "id": "Qh-fT7-lusk8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import InputExample\n",
        "\n",
        "def example_create_fn(row):\n",
        "  return InputExample(\n",
        "      texts=[row[\"title\"]]\n",
        "  )\n",
        "\n",
        "examples = sub_ds.apply(lambda x: example_create_fn(x), axis=1).tolist()\n",
        "examples[:10]"
      ],
      "metadata": {
        "id": "5DtmP7U8u5OB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "titles_list = sub_ds[\"title\"].tolist()\n",
        "\n",
        "titles_embedding = model.encode(titles_list)\n",
        "\n",
        "len(titles_embedding), len(titles_embedding[0])"
      ],
      "metadata": {
        "id": "GreoOAQs4Vlw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "id_index = sub_ds.index\n",
        "faiss.normalize_L2(titles_embedding)\n",
        "\n",
        "index_content = faiss.IndexIDMap(faiss.IndexFlatIP(len(titles_embedding[0])))\n",
        "index_content.add_with_ids(titles_embedding, id_index)"
      ],
      "metadata": {
        "id": "aOgWJCrb73tX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def search_content(query, pdf_to_index, k=3):\n",
        "    query_vector = model.encode([query]) # Encode the query string into an embedding vector.\n",
        "    faiss.normalize_L2(query_vector)  # Normalize the query vector.\n",
        "\n",
        "    # Perform the search\n",
        "    top_k = k\n",
        "    similarities, indices = index_content.search(query_vector, top_k)  # Similarity scores and indices for the matches.\n",
        "\n",
        "    results = pdf_to_index.loc[indices.flatten()]  # Retrieve the matching articles from pdf_to_index.\n",
        "    results[\"similarities\"] = similarities.flatten()  # Add similarity scores.\n",
        "    return results\n",
        "\n",
        "display(search_content(\"animal\", sub_ds, k=5))"
      ],
      "metadata": {
        "id": "SybXbdAiLxNZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chroma_client = chromadb.Client()\n",
        "collection_name = \"my_news\"\n",
        "\n",
        "# If a collection with the same name exists, delete it to avoid conflicts\n",
        "if len(chroma_client.list_collections()) > 0 and collection_name in [chroma_client.list_collections()[0].name]:\n",
        "    chroma_client.delete_collection(name=collection_name)\n",
        "\n",
        "print(f\"Creating collection: '{collection_name}'\")\n",
        "collection = chroma_client.create_collection(name=collection_name)"
      ],
      "metadata": {
        "id": "AaMnn8oSYxbL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the DataFrame subset (for reference)\n",
        "display(sub_ds)\n",
        "\n",
        "collection.add(\n",
        "    documents=sub_ds[\"title\"][:100].tolist(),\n",
        "    metadatas=[{\"topic\": topic} for topic in sub_ds[\"topic\"][:100].tolist()],\n",
        "    ids=[str(id) for id in sub_ds.index[:100].tolist()]\n",
        ")"
      ],
      "metadata": {
        "id": "K19SiQEfZ9Xn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "results = collection.query(\n",
        "    query_texts=[\"space development\"],\n",
        "    n_results=2\n",
        ")\n",
        "\n",
        "print(json.dumps(results, indent=4))"
      ],
      "metadata": {
        "id": "164S9gO3aehH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = \"gpt2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "lm_model = AutoModelForCausalLM.from_pretrained(model_id)\n",
        "\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=lm_model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=512,\n",
        "    device_map=\"auto\",\n",
        ")"
      ],
      "metadata": {
        "id": "guKrNzLObStU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What's the latest news on space development?\"\n",
        "context = \" \".join([f\"#{str(i)}\" for i in results[\"documents\"][0]])\n",
        "prompt_template = f\"Relevant context: {context}\\n\\n The user's question: {question}\""
      ],
      "metadata": {
        "id": "LxiHZ9H9cBdr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lm_response = pipe(prompt_template)\n",
        "print(lm_response[0][\"generated_text\"])"
      ],
      "metadata": {
        "id": "cDKYN8RoeXLJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}