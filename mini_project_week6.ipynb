{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP0hd9BepmZeIlE1Uu1zIEr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LeibGit/-DI_Bootcamp/blob/main/mini_project_week6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "zip_file_path = \"cats_dogs.zip\"\n",
        "destination_path = \"data/cats_dogs\"\n",
        "\n",
        "try:\n",
        "  with zipfile.ZipFile(zip_file_path, \"r\") as zip_ref:\n",
        "    zip_ref.extractall(destination_path)\n",
        "    print(\"File successfully extracted.\")\n",
        "except Exception as e:\n",
        "  print(\"An error occured extracting\", e)"
      ],
      "metadata": {
        "id": "U9uY-_FE_MPJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "itjr4fis6Zqc"
      },
      "outputs": [],
      "source": [
        "# Prefilled. Just copy and execute.\n",
        "import os, math, re, random\n",
        "from glob import glob\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "np.random.seed(42); tf.random.set_seed(42)\n",
        "\n",
        "# Paths - change if needed\n",
        "DATA_ROOT = Path(\"data/cats_dogs\")\n",
        "train_dir = (DATA_ROOT / \"train\" / \"train\") if (DATA_ROOT / \"train\" / \"train\").exists() else (DATA_ROOT / \"train\")\n",
        "test_dir  = (DATA_ROOT / \"test\"  / \"test\")  if (DATA_ROOT / \"test\"  / \"test\").exists()  else (DATA_ROOT / \"test\")\n",
        "\n",
        "IMG_HEIGHT, IMG_WIDTH = 180, 180\n",
        "batch_size = 32\n",
        "seed = 1337\n",
        "\n",
        "# Build DataFrames from folders\n",
        "def build_df_from_folder(folder: Path, labeled: bool=True):\n",
        "    exts = ('*.jpg','*.jpeg','*.png','*.bmp')\n",
        "    files = []\n",
        "    for ex in exts:\n",
        "        files.extend(glob(str(folder / '**' / ex), recursive=True))\n",
        "    if not files:\n",
        "        raise FileNotFoundError(f\"No images found under {folder}\")\n",
        "    rows = []\n",
        "    for f in files:\n",
        "        if labeled:\n",
        "            name = Path(f).name.lower()\n",
        "            parent = Path(f).parent.name.lower()\n",
        "            if parent in {\"cat\",\"cats\"}:\n",
        "                label = \"cat\"\n",
        "            elif parent in {\"dog\",\"dogs\"}:\n",
        "                label = \"dog\"\n",
        "            else:\n",
        "                if re.search(r'(^|[^a-z])cat([^a-z]|$)', name): label = \"cat\"\n",
        "                elif re.search(r'(^|[^a-z])dog([^a-z]|$)', name): label = \"dog\"\n",
        "                else:\n",
        "                    continue\n",
        "            rows.append({\"filepath\": f, \"label\": label})\n",
        "        else:\n",
        "            rows.append({\"filepath\": f})\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "df_train_full = build_df_from_folder(train_dir, labeled=True)\n",
        "df_test_full  = build_df_from_folder(test_dir,  labeled=False)\n",
        "\n",
        "# Train validation split\n",
        "from sklearn.model_selection import train_test_split\n",
        "df_tr, df_val = train_test_split(\n",
        "    df_train_full, test_size=0.2, stratify=df_train_full[\"label\"], random_state=seed\n",
        ")\n",
        "\n",
        "# Generators\n",
        "train_gen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=10,\n",
        "    zoom_range=0.1,\n",
        "    horizontal_flip=True,\n",
        ")\n",
        "# No augmentation generator\n",
        "na_train_gen = ImageDataGenerator()\n",
        "\n",
        "val_gen = ImageDataGenerator(rescale=1./255)\n",
        "test_gen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_flow = train_gen.flow_from_dataframe(\n",
        "    df_tr, x_col=\"filepath\", y_col=\"label\",\n",
        "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    class_mode=\"binary\", batch_size=batch_size,\n",
        "    shuffle=True, seed=seed, validate_filenames=False\n",
        ")\n",
        "val_flow = val_gen.flow_from_dataframe(\n",
        "    df_val, x_col=\"filepath\", y_col=\"label\",\n",
        "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    class_mode=\"binary\", batch_size=batch_size,\n",
        "    shuffle=False, validate_filenames=False\n",
        ")\n",
        "# Unlabeled test for inference only\n",
        "test_flow = test_gen.flow_from_dataframe(\n",
        "    df_test_full, x_col=\"filepath\", y_col=None,\n",
        "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    class_mode=None, batch_size=batch_size,\n",
        "    shuffle=False, validate_filenames=False\n",
        ")\n",
        "\n",
        "print({\"train\": train_flow.samples, \"val\": val_flow.samples, \"test\": test_flow.samples,\n",
        "       \"class_indices\": train_flow.class_indices})"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prefilled. Just copy and execute.\n",
        "import os, math, re, random\n",
        "from glob import glob\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "np.random.seed(42); tf.random.set_seed(42)\n",
        "\n",
        "# Paths - change if needed\n",
        "DATA_ROOT = Path(\"data/cats_dogs\")\n",
        "train_dir = (DATA_ROOT / \"train\" / \"train\") if (DATA_ROOT / \"train\" / \"train\").exists() else (DATA_ROOT / \"train\")\n",
        "test_dir  = (DATA_ROOT / \"test\"  / \"test\")  if (DATA_ROOT / \"test\"  / \"test\").exists()  else (DATA_ROOT / \"test\")\n",
        "\n",
        "IMG_HEIGHT, IMG_WIDTH = 180, 180\n",
        "batch_size = 32\n",
        "seed = 1337\n",
        "\n",
        "# Build DataFrames from folders\n",
        "def build_df_from_folder(folder: Path, labeled: bool=True):\n",
        "    exts = ('*.jpg','*.jpeg','*.png','*.bmp')\n",
        "    files = []\n",
        "    for ex in exts:\n",
        "        files.extend(glob(str(folder / '**' / ex), recursive=True))\n",
        "    if not files:\n",
        "        raise FileNotFoundError(f\"No images found under {folder}\")\n",
        "    rows = []\n",
        "    for f in files:\n",
        "        if labeled:\n",
        "            name = Path(f).name.lower()\n",
        "            parent = Path(f).parent.name.lower()\n",
        "            if parent in {\"cat\",\"cats\"}:\n",
        "                label = \"cat\"\n",
        "            elif parent in {\"dog\",\"dogs\"}:\n",
        "                label = \"dog\"\n",
        "            else:\n",
        "                if re.search(r'(^|[^a-z])cat([^a-z]|$)', name): label = \"cat\"\n",
        "                elif re.search(r'(^|[^a-z])dog([^a-z]|$)', name): label = \"dog\"\n",
        "                else:\n",
        "                    continue\n",
        "            rows.append({\"filepath\": f, \"label\": label})\n",
        "        else:\n",
        "            rows.append({\"filepath\": f})\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "df_train_full = build_df_from_folder(train_dir, labeled=True)\n",
        "df_test_full  = build_df_from_folder(test_dir,  labeled=False)\n",
        "\n",
        "# Train validation split\n",
        "from sklearn.model_selection import train_test_split\n",
        "df_tr, df_val = train_test_split(\n",
        "    df_train_full, test_size=0.2, stratify=df_train_full[\"label\"], random_state=seed\n",
        ")\n",
        "\n",
        "\n",
        "# No augmentation generator\n",
        "na_train_gen = ImageDataGenerator()\n",
        "\n",
        "val_gen = ImageDataGenerator(rescale=1./255)\n",
        "test_gen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "na_train_flow = na_train_gen.flow_from_dataframe(\n",
        "    df_tr, x_col=\"filepath\", y_col=\"label\",\n",
        "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    class_mode=\"binary\", batch_size=batch_size,\n",
        "    shuffle=True, seed=seed, validate_filenames=False\n",
        ")\n",
        "\n",
        "val_flow = val_gen.flow_from_dataframe(\n",
        "    df_val, x_col=\"filepath\", y_col=\"label\",\n",
        "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    class_mode=\"binary\", batch_size=batch_size,\n",
        "    shuffle=False, validate_filenames=False\n",
        ")\n",
        "\n",
        "# Unlabeled test for inference only\n",
        "test_flow = test_gen.flow_from_dataframe(\n",
        "    df_test_full, x_col=\"filepath\", y_col=None,\n",
        "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    class_mode=None, batch_size=batch_size,\n",
        "    shuffle=False, validate_filenames=False\n",
        ")\n",
        "\n",
        "print({\"train\": train_flow.samples, \"val\": val_flow.samples, \"test\": test_flow.samples, \"class_indices\": train_flow.class_indices})"
      ],
      "metadata": {
        "id": "1zn4UUpSeqlp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "CThbjRWF7ECE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To-Do: Describe your planned CNN in full sentences before coding:\n",
        "\n",
        "Number of convolutional blocks and filter sizes.\n",
        "1. Going to use 3 conv2d layers(blocks) the first one being 32 batches with (3, 3) and the others using the same (3, 3) but being in 64 batches.\n",
        "Placement of MaxPooling to reduce spatial dimensions.\n",
        "We will do 2 maxpools one after the 32 first conv2d layer and one after the second conv2d layer. this will be (2, 2)\n",
        "Use of Dropout and why it helps with regularization.\n",
        "Dropout should be set at 20% and is helpfull for reducing overfitting and over  relience on particular nuerons.\n",
        "Final Dense layers including the output layer with a single sigmoid unit for binary targets.\n",
        "The best option would be to use one nueron with sigmoid activation, since this is a binary classification."
      ],
      "metadata": {
        "id": "EuRKpWiFfSNE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To-Do: Specify and justify\n",
        "\n",
        "Optimizer choice, recommended Adam for fast convergence.\n",
        "I am going to go with adam for faster convergence.\n",
        "Initial learning rate and why it is reasonable.\n",
        "I will go with 0.001 since this is usually the default for the adam optimizer.\n",
        "Batch size relative to GPU or CPU memory.\n",
        "I will be using 32 and 64 batch size since this is standard.\n",
        "EarlyStopping on validation loss and optional ReduceLROnPlateau to adapt learning rate.\n"
      ],
      "metadata": {
        "id": "M_pXQXk7o7Po"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To-Do: Train for a fixed number of epochs, log training and validation curves, and then repeat with early stopping enabled.\n",
        "\n",
        "Explain how you detect overfitting from the curves and what change you make to mitigate it.\n",
        "\n",
        "Why: Curves reveal the bias variance tradeoff. Divergence between train and validation indicates overfitting. Mitigations include stronger augmentation, more dropout, or fewer parameters."
      ],
      "metadata": {
        "id": "alFQ7S7KqT6d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import models, layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(32, (3, 3), activation=\"relu\", input_shape=(180, 180, 3)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation=\"relu\"))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation=\"relu\"))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(64, activation=\"relu\"))\n",
        "model.add(layers.Dense(units=1, activation='sigmoid'))\n",
        "\n",
        "model.compile(\n",
        "    optimizer=\"adam\",\n",
        "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    train_flow,\n",
        "    epochs=10,\n",
        "    validation_data=val_flow\n",
        ")"
      ],
      "metadata": {
        "id": "G6D_KFUpqZgT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot and evaluate\n",
        "plt.plot(history.history['accuracy'], label='accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='val_accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()\n",
        "test_loss, test_acc = model.evaluate(val_flow)\n",
        "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_acc}\")"
      ],
      "metadata": {
        "id": "JV3cRdoh0Qd_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Changes that should be made is not possivly the learning rate. Additionally, the image data generator is changing the image too much. The large variance in accuracy indicates overfitting."
      ],
      "metadata": {
        "id": "k34hjopIFexs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "val_probs = model.predict(val_flow)\n",
        "val_preds = (val_probs > 0.5).astype(int).ravel()\n",
        "val_true = val_flow.classes\n",
        "\n",
        "cm = confusion_matrix(val_true, val_preds)\n",
        "disp = ConfusionMatrixDisplay(\n",
        "    confusion_matrix=cm,\n",
        "    display_labels=list(train_flow.class_indices.keys())\n",
        ")\n",
        "disp.plot(cmap=\"Blues\")"
      ],
      "metadata": {
        "id": "z8K2IOprI1Lq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# No augmentation model\n",
        "from tensorflow.keras import models, layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "model_na = models.Sequential()\n",
        "model_na.add(layers.Conv2D(32, (3, 3), activation=\"relu\", input_shape=(180, 180, 3)))\n",
        "model_na.add(layers.MaxPooling2D((2, 2)))\n",
        "model_na.add(layers.Conv2D(64, (3, 3), activation=\"relu\"))\n",
        "model_na.add(layers.MaxPooling2D((2, 2)))\n",
        "model_na.add(layers.Conv2D(64, (3, 3), activation=\"relu\"))\n",
        "model_na.add(layers.Flatten())\n",
        "model_na.add(layers.Dense(64, activation=\"relu\"))\n",
        "model_na.add(layers.Dense(units=1, activation='sigmoid'))\n",
        "\n",
        "model_na.compile(\n",
        "    optimizer=\"adam\",\n",
        "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "history = model_na.fit(\n",
        "    na_train_flow,\n",
        "    epochs=10,\n",
        "    validation_data=val_flow\n",
        ")"
      ],
      "metadata": {
        "id": "9sVFTn_gja69"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot and evaluate\n",
        "plt.plot(history.history['accuracy'], label='accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='val_accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()\n",
        "test_loss, test_acc = model_na.evaluate(val_flow)\n",
        "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_acc}\")"
      ],
      "metadata": {
        "id": "onZI5dXqj2Yr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "val_probs = model.predict(val_flow)\n",
        "val_preds = (val_probs > 0.5).astype(int).ravel()\n",
        "val_true = val_flow.classes\n",
        "\n",
        "cm = confusion_matrix(val_true, val_preds)\n",
        "disp = ConfusionMatrixDisplay(\n",
        "    confusion_matrix=cm,\n",
        "    display_labels=list(train_flow.class_indices.keys())\n",
        ")\n",
        "disp.plot(cmap=\"Blues\")"
      ],
      "metadata": {
        "id": "zzgeLqxtj-Iv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Without augmentation, the model overfit the data. This can be seen by how well the model performed on the training data, but how poorly it performed on the testing data."
      ],
      "metadata": {
        "id": "JTWSHO0cp6YB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils import class_weight\n",
        "import numpy as np\n",
        "\n",
        "class_weights = class_weight.compute_class_weight(\n",
        "  class_weight='balanced',\n",
        "  classes=np.unique(train_flow.classes),\n",
        "  y=train_flow.classes\n",
        ")\n",
        "\n",
        "class_weight_dict = dict(enumerate(class_weights))\n",
        "print(class_weight_dict)"
      ],
      "metadata": {
        "id": "tEv_tkI0q7SW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# saving model\n",
        "model.save(\"model.keras\")\n",
        "\n",
        "model_config = model.to_json()\n",
        "\n",
        "with open(\"model_config.json\", \"w\") as json_file:\n",
        "  json_file.write(model_config)"
      ],
      "metadata": {
        "id": "h0DP97sYtlNC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To-Do: Propose one of the following and justify the expected benefit\n",
        "\n",
        "Batch Normalization after convolutions.\n",
        "Transfer learning with a frozen lightweight backbone like MobileNetV2 and a small classifier head.\n",
        "Mixed precision training to accelerate on modern GPUs.\n",
        "Why: These techniques can yield better accuracy or faster training with limited additional code.I believe using mixed precision training on modern GPU's since this will also help the model run faster."
      ],
      "metadata": {
        "id": "VPtxU15kvgCJ"
      }
    }
  ]
}